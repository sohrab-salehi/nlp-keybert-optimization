# NLP Project: Optimizing KeyBERT for Advanced Keyword Extraction âœ¨

## Overview ğŸ”¬
This project investigates keyword extraction using KeyBERT with optimized hyperparameters. The study enhances evaluation metrics to accommodate partial matches, significantly improving precision. By systematically fine-tuning hyperparameters and performing model selection, KeyBERT is optimized on the SemEval dataset and its adaptability is assessed through transfer learning on the KPTimes dataset.

## Features ğŸš€
- Preprocess textual data by removing extraneous elements such as links, mentions, hashtags, white spaces, and punctuation.
- Optimize KeyBERTâ€™s hyperparameters for enhanced keyword prediction accuracy.
- Evaluate and compare multiple transformer architectures for keyword extraction.
- Implement Maximum Marginal Relevance (MMR) to improve keyword selection diversity and relevance.
- Examine cross-domain generalization using transfer learning on the KPTimes dataset.

## Datasets ğŸ“Š
The project leverages two distinct datasets:

1. **SemEval-2010 Task 8 Dataset**
   - Contains 8,000 training samples and 2,700 test samples.
   - Expert-annotated entities with high-quality linguistic annotations.
   - Average sentence length of approximately 120 words.
   - Downloaded from Hugging Face using the `load_dataset` method:
   ```python
   from datasets import load_dataset
   dataset = load_dataset("semeval2010_task8")
   ```

2. **KPTimes Dataset (Sports Domain)**
   - Comprises 260,000 training samples and 20,000 test samples.
   - Consists of long-form news articles with complex syntactic structures.
   - Average sentence length of approximately 500 words.
   - The dataset was obtained from [this repository](https://github.com/ygorg/KPTimes).

## Installation âš™ï¸
To set up the project environment:

1. Clone the repository:
   ```sh
   git clone https://github.com/your-username/nlp-keybert-optimization.git
   cd nlp-keybert-optimization
   ```

2. Create and activate a virtual environment (optional but recommended):
   ```sh
   python -m venv venv
   source venv/bin/activate  # On Windows use: venv\Scripts\activate
   ```

3. Install required dependencies:
   ```sh
   pip install -r requirements.txt
   ```

### Running on Google Colab ğŸ’»
Certain Jupyter notebooks within this project are configured for execution on **Google Colab**. These notebooks may include:
- `!pip install` commands for installing dependencies within the Colab environment.
- `from google.colab import drive` for accessing Google Drive storage.
- Environment setup commands specific to Colab.

For execution on a local machine, these Colab-specific commands should be removed or modified accordingly.

## Project Structure ğŸ“‚
```
â”œâ”€â”€ results/           # includes all the running results in csv format
â”œâ”€â”€ notebooks/           # Jupyter notebooks (see details below)
â”‚   â”œâ”€â”€ baseline.ipynb
â”‚   â”œâ”€â”€ baseline_new_eval.ipynb
â”‚   â”œâ”€â”€ MMR.ipynb
â”‚   â”œâ”€â”€ maxsum.ipynb
|   â”œâ”€â”€ results.ipynb
â”‚   â”œâ”€â”€ model_exploration_maxsum.ipynb
â”‚   â”œâ”€â”€ model_exploration_mmr.ipynb
â”‚   â”œâ”€â”€ transfer_learning.ipynb
â”œâ”€â”€ requirements.txt     # Project dependencies
â”œâ”€â”€ README.md            # Documentation
```

### Notebooks ğŸ““
The `notebooks/` folder contains Jupyter notebooks dedicated to different tasks:
1. **baseline**: Initial KeyBERT run with a strict evaluation method.
2. **baseline_new_eval**: KeyBERT run results after modifying the evaluation function.
3. **MMR**: Fine-tuning hyperparameters using Maximum Marginal Relevance (MMR).
4. **maxsum**: Fine-tuning hyperparameters using MaxSum selection.
5. **results**: Running results and data visualization.
6. **model_exploration_maxsum**: Model exploration with MaxSum, running KeyBERT with different input models.
7. **model_exploration_mmr**: Same as 6 but using MMR.
8. **transfer_learning**: KeyBERT run on the KPTimes dataset (sports category) with the best hyperparameters from the best SemEval run.

## Results ğŸ“ˆ
- **Evaluation Metrics**: Precision increased from 7% to 42% by incorporating partial matches.
- **Hyperparameter Optimization**: Achieved a peak **79.5% F1-score** using MMR.
- **Transformer Model Performance**:
  - Sentence-Transformers/Paraphrase-MiniLM-L6-v2 attained **80% precision** on the SemEval dataset.
  - Comparative analysis of alternative models including RoBERTa, ALBERT, and MPNet.
- **Transfer Learning Analysis**:
  - The optimized model was applied to the KPTimes dataset (sports domain).
  - Performance degradation observed due to longer sentence structures, with precision decreasing to 18%.

## Future Research Directions ğŸ”
- Explore segmentation techniques to improve performance on lengthy texts.
- Conduct domain-specific fine-tuning for enhanced adaptability.
- Investigate newer transformer architectures (e.g., GPT, T5) for keyword extraction tasks.
- Leverage Sentence-BERT (SBERT) for improved contextual keyword extraction.
- Incorporate stemming techniques to enhance keyword alignment accuracy.

## Contributing ğŸ¤
Contributions are encouraged! To participate, open an issue or submit a pull request.

## License ğŸ“œ
This project is distributed under the MIT License.
